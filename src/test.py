import pandas as pd
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt
import faiss
import asyncio
import torch
import os
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.readers import InputExample
from sklearn.cluster import KMeans
from sqlalchemy import create_engine


def train():
    model = SentenceTransformer(os.getenv("EMBEDDINGS_MODEL"))

    # Step 2ì—ì„œ ë§Œë“  ë°ì´í„°ë¡œ InputExample ë¦¬ìŠ¤íŠ¸ ìƒì„±
    train_examples = [
        InputExample(texts=['íŒŒì´ì¬ ê°•ì˜ ì„¤ëª… 1', 'ìë£Œêµ¬ì¡° ê°•ì˜ ì„¤ëª… 1'], label=1.0),
        InputExample(texts=['íŒŒì´ì¬ ê°•ì˜ ì„¤ëª… 1', 'ìë°” ê°•ì˜ ì„¤ëª… 1'], label=0.0),
    ]

    # EmbeddingSimilarityEvaluator
    # - ë§¤ ì—í­(epoch)ì´ ëë‚  ë•Œë§ˆë‹¤ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ìë™ ì €ì¥í•´ì¤Œ
    evaluator = EmbeddingSimilarityEvaluator.from_input_examples(train_examples, name='sts-dev')

    # DataLoader ë° í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)

    # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
    # - MultipleNegativesRankingLoss: í•œ ë°°ì¹˜ ë‚´ì—ì„œ (ì§ˆë¬¸, ì •ë‹µNCS) ìŒì˜ ìœ ì‚¬ë„ëŠ” ë†’ì´ê³ ,
    # - ë‚˜ë¨¸ì§€ (ì§ˆë¬¸, ë‹¤ë¥¸NCS) ìŒë“¤ì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶”ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. ê²€ìƒ‰/ì¶”ì²œì— ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.
    train_loss = losses.MultipleNegativesRankingLoss(model)

    # 4. ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    # - ì „ì²´ ìŠ¤í…ì˜ 10%ë¥¼ ì›Œë°ì—…ìœ¼ë¡œ ì‚¬ìš©
    epochs = 4
    warmup_steps = int(len(train_dataloader) * epochs * 0.1)

    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=epochs,
        warmup_steps=warmup_steps,
        evaluator=evaluator,
        output_path='./my-online-edu-recommender'
    )


if __name__ == "__main__":
    model = SentenceTransformer("./my-online-edu-recommender")






def load_data_from_db_with_embeddings(engine):
    try:
        query = """
            SELECT
                unit.comp_unit_id,
                unit.comp_unit_name,
                unit.comp_unit_def,
                unit.comp_unit_level,
                emb.embedding_unit_def
            FROM tb_ncs_comp_unit_emb emb
            JOIN tb_ncs_comp_unit unit
            ON emb.comp_unit_id = unit.comp_unit_id
            WHERE unit.useflag = 'Y' AND unit.use_ncs = 'Y'
        """

        df = pd.read_sql(query, engine)
        print(f"ì„±ê³µì ìœ¼ë¡œ {len(df)}ê±´ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

        # ì¤‘ìš”: í…ìŠ¤íŠ¸ í˜•íƒœì˜ ì„ë² ë”©ì„ ì‹¤ì œ Numpy ë°°ì—´ë¡œ ë³€í™˜
        # ì˜ˆ: "[0.1,0.2,...]" -> np.array([0.1, 0.2, ...])
        # DB ì €ì¥ í˜•ì‹ì— ë”°ë¼ í›„ì²˜ë¦¬ ë°©ì‹(sep ë“±)ì„ ì¡°ì ˆí•´ì•¼ í•©ë‹ˆë‹¤.
        df['embedding_unit_def'] = df['embedding_unit_def'].apply(
            lambda x: np.fromstring(x.strip('[]'), sep=',')
        )

        return df

    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None



# --- 1ë‹¨ê³„: ê¸°ë°˜ ì‘ì—… (ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ê·¸ë˜í”„ êµ¬ì¶•) ---



def build_skill_graph(data: pd.DataFrame, k_neighbors=20, n_probes=10):
    """
    Pandas DataFrame í˜•íƒœì˜ NCS ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—­ëŸ‰ ê´€ê³„ë§(ê·¸ë˜í”„) êµ¬ì¶•
    FAISSë¥¼ ì‚¬ìš©í•´ì„œ ë¸Œë£¨íŠ¸ í¬ìŠ¤ ë°©ì‹ì´ ì•„ë‹Œ ANN ê¸°ë°˜ìœ¼ë¡œ ì‹œê°„ ë³µì¡ë„ ê°œì„ 
    """

    # 1. DBì—ì„œ ë¶ˆëŸ¬ì˜¨ ì„ë² ë”© ë²¡í„°ë¥¼ Faissê°€ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    # - (ë°ì´í„° ê°œìˆ˜, ì„ë² ë”© ì°¨ì›) í˜•íƒœì˜ 2D Numpy ë°°ì—´
    # - float32 ìë£Œí˜•
    embedding_unit_def = np.array(list(data['embedding_unit_def'])).astype('float32')
    embedding_dimension = embedding_unit_def.shape[1]

    # 2. Faiss ì¸ë±ìŠ¤ ìƒì„± ë° ë²¡í„° ì¶”ê°€
    # IndexFlatL2ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ L2 ê±°ë¦¬(ìœ í´ë¦¬ë“œ ê±°ë¦¬) ê¸°ë°˜ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ L2 ê±°ë¦¬ëŠ” ì •ê·œí™”ëœ(normalized) ë²¡í„°ì—ì„œëŠ” ë™ì¼í•œ ìˆœì„œë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    quantizer = faiss.IndexFlatIP(embedding_dimension)
    nlist = int(np.sqrt(len(embedding_unit_def)))
    index = faiss.IndexIVFFlat(quantizer, embedding_dimension, nlist, faiss.METRIC_INNER_PRODUCT)

    print("Faiss ì¸ë±ìŠ¤ë¥¼ í•™ìŠµ(training)í•©ë‹ˆë‹¤...")
    index.train(embedding_unit_def)

    print("Faiss ì¸ë±ìŠ¤ì— ë²¡í„°ë¥¼ ì¶”ê°€(add)í•©ë‹ˆë‹¤...")
    index.add(embedding_unit_def)

    # nprobe íŒŒë¼ë¯¸í„° ì„¤ì •
    index.nprobe = n_probes

    print(f"Faiss ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ. ì´ {index.ntotal}ê°œì˜ ë²¡í„°ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 3. ê° ë²¡í„°ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ì´ì›ƒ ê²€ìƒ‰
    # search(ìê¸° ìì‹ ì„ í¬í•¨í•œ ëª¨ë“  ë²¡í„°, ì°¾ì„ ì´ì›ƒì˜ ìˆ˜ k+1)
    # ìê¸° ìì‹ ë„ ê²°ê³¼ì— í¬í•¨ë˜ë¯€ë¡œ k+1ê°œ ê²€ìƒ‰
    distances, indices = index.search(embedding_unit_def, k_neighbors + 1)

    # ë°©í–¥ì„± ìˆëŠ” ê·¸ë˜í”„(DiGraph) ìƒì„±
    G = nx.DiGraph()

    # ê·¸ë˜í”„ì— ë…¸ë“œ(ëŠ¥ë ¥ë‹¨ìœ„) ì¶”ê°€
    for index, row in data.iterrows():
        G.add_node(row['comp_unit_id'], name=row['comp_unit_name'], level=row['comp_unit_level'])

    # 3. ì—£ì§€ ìƒì„± ì‹œ, ë¯¸ë¦¬ ê³„ì‚°ëœ ì„ë² ë”©ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚°
    for i in tqdm(range(len(data)), desc="ì—£ì§€ ìƒì„± ì¤‘..."):
        level_i = int(data.loc[i, 'comp_unit_level'])

        for neighbor_idx, dist in zip(indices[i], distances[i]):
            if i == neighbor_idx or neighbor_idx < 0:
                continue

            level_j = int(data.loc[neighbor_idx, 'comp_unit_level'])

            if level_i >= level_j:
                continue

            # NCS ë ˆë²¨ ì°¨ì´ê°€ í´ìˆ˜ë¡ íŒ¨ë„í‹° ë¶€ì—¬ (ìš°ì„  ìˆœìœ„ì—ì„œ ë°€ë ¤ë‚¨)
            level_gap_penalty = (level_j - level_i - 1) * 0.5
            weight = (1 - dist) + level_gap_penalty

            G.add_edge(data.loc[i, 'comp_unit_id'], data.loc[neighbor_idx, 'comp_unit_id'], weight=weight)

    return G



# --- 2ë‹¨ê³„: ê²½ë¡œ íƒìƒ‰ ë° ê°­ ë¶„ì„ ---

def find_career_path(graph: nx.DiGraph, current_skill_ids: list, target_skill_id: str):
    """
    ì£¼ì–´ì§„ ê·¸ë˜í”„ì—ì„œ í˜„ì¬ ì—­ëŸ‰ìœ¼ë¡œë¶€í„° ëª©í‘œ ì—­ëŸ‰ê¹Œì§€ì˜ ìµœì  ê²½ë¡œì™€ ìŠ¤í‚¬ ê°­ì„ ì°¾ìŠµë‹ˆë‹¤.
    """

    best_path = None
    min_path_cost = float('inf')

    # í˜„ì¬ ë³´ìœ í•œ ì—¬ëŸ¬ ìŠ¤í‚¬ ì¤‘ ì–´ë–¤ ìŠ¤í‚¬ì—ì„œ ì¶œë°œí•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì ì¸ì§€ íƒìƒ‰
    for start_id in current_skill_ids:
        try:
            # ë‹¤ìµìŠ¤íŠ¸ë¼ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìµœë‹¨ ê²½ë¡œ(ìµœì†Œ ë¹„ìš© ê²½ë¡œ) íƒìƒ‰
            path = nx.dijkstra_path(graph, source=start_id, target=target_skill_id, weight='weight')
            path_cost = nx.dijkstra_path_length(graph, source=start_id, target=target_skill_id, weight='weight')

            if path_cost < min_path_cost:
                min_path_cost = path_cost
                best_path = path

        except nx.NetworkXNoPath:
            # ê²½ë¡œê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë¬´ì‹œ
            continue

    if best_path is None:
        return None, None # ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°

    # ìŠ¤í‚¬ ê°­ = ì¶”ì²œ ê²½ë¡œ ì¤‘ í˜„ì¬ ë³´ìœ  ì—­ëŸ‰ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€
    skill_gap = [skill_id for skill_id in best_path if skill_id not in current_skill_ids]

    return best_path, skill_gap



def get_course_from_node(comp_unit_id, engine):
    # 1. íŒŒë¼ë¯¸í„°í™”ëœ ì¿¼ë¦¬ ì‚¬ìš© (SQL Injection ë°©ì§€ ë° êµ¬ë¬¸ ì˜¤ë¥˜ í•´ê²°)
    # - SQL ì¿¼ë¦¬ë¬¸ ì•ˆì—ëŠ” í”Œë ˆì´ìŠ¤í™€ë”(%(ë³€ìˆ˜ëª…)s)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    query = """
    SELECT sbj_nm
    FROM tb_cos_m
    WHERE cos_id IN (SELECT cos_id
    FROM tb_cos_ncs_mat_d mat
    WHERE mat.comp_unit_id = %(comp_id)s
    AND mat.useflag = 'Y'
    AND mat.del_yn = 'N'
    AND mat.match_rate >= 75
    ORDER BY mat.match_rate DESC
    LIMIT 10
    ) AND useflag = 'Y' AND del_yn = 'N'
    """

    # - params ë”•ì…”ë„ˆë¦¬ì— ì‹¤ì œ ê°’ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
    params = {'comp_id': comp_unit_id}

    # - read_sql í˜¸ì¶œ ì‹œ engineê³¼ í•¨ê»˜ paramsë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
    df = pd.read_sql(query, engine, params=params)

    # 2. DataFrameì˜ íŠ¹ì • ì»¬ëŸ¼ ë‚´ìš©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (ë¡œì§ ì˜¤ë¥˜ í•´ê²°)
    if df.empty:
        return []

    # dict.fromkeys()ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
    sbj_nm_list = df['sbj_nm'].tolist()
    return list(dict.fromkeys(sbj_nm_list))



if __name__ == "__main__":
    db_url = "postgresql://u_cufit:cufit23$@223.130.152.134:5432/curationdb"
    engine = create_engine(db_url)
    ncs_df = load_data_from_db_with_embeddings(engine)

    # 1. ì—­ëŸ‰ ê´€ê³„ë§ ê·¸ë˜í”„ ìƒì„±
    print("Step 1: ì—­ëŸ‰ ê´€ê³„ë§ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
    skill_graph = build_skill_graph(ncs_df)
    print(f"ê·¸ë˜í”„ êµ¬ì¶• ì™„ë£Œ! ë…¸ë“œ: {skill_graph.number_of_nodes()}ê°œ, ì—£ì§€: {skill_graph.number_of_edges()}ê°œ\n")

    if ncs_df is not None and not ncs_df.empty:
        while True:
            comp_unit_id = input("ëŠ¥ë ¥ë‹¨ìœ„ ID: ")
            target_comp_unit_id = input("íƒ€ê²Ÿ ID: ")

            # 2. ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
            # ì‹œë‚˜ë¦¬ì˜¤: í˜„ì¬ ë‚˜ëŠ” ì–´íšë¬¼ì„ ë¶„ë¥˜í•˜ê³  ìš´ë°˜í•˜ëŠ” ì—­ëŸ‰ì„ ê°€ì§€ê³  ìˆì–´ -> ì–´ë¥˜ ì¢…ìë¥¼ ìƒì‚°í•˜ëŠ” ëŠ¥ë ¥ì„ í‚¤ìš°ê³  ì‹¶ì–´
            # - ì‹œì‘: 11.ì–´íšë¬¼ ë¶„ë¥˜ Â· ìš´ë°˜ (NCS Level 2) <- ë‚´ê°€ í˜„ì¬ ê°€ì§„ ì—­ëŸ‰
            # - íƒ€ê²Ÿ: 05.ì–´ë¥˜ ì¢…ììƒì‚° (NCS Level 5) <- ë‚˜ì˜ ëª©í‘œ ì—­ëŸ‰
            my_skills = ['2404010411']
            target_skill = '2404020205'

            my_skills = [comp_unit_id]
            target_skill = target_comp_unit_id

            print("Step 2: ìµœì  ê²½ë¡œ ë° ìŠ¤í‚¬ ê°­ì„ ë¶„ì„í•©ë‹ˆë‹¤...")
            print(f"â–¶ í˜„ì¬ ë³´ìœ  ì—­ëŸ‰: {[skill_graph.nodes[s]['name'] for s in my_skills]}")
            print(f"â–¶ ëª©í‘œ ì—­ëŸ‰: {skill_graph.nodes[target_skill]['name']}\n")

            # 3. ê²½ë¡œ íƒìƒ‰ ë° ê²°ê³¼ ì¶œë ¥
            recommended_path, skill_gap = find_career_path(skill_graph, my_skills, target_skill)

            if recommended_path:
                print("Step 3: ë¶„ì„ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤!\n")
                print("="*70)
                print("ğŸš€ ë‹¹ì‹ ì„ ìœ„í•œ ì¶”ì²œ ì„±ì¥ ë¡œë“œë§µ ğŸš€")
                print("="*70)

                for i, skill_id in enumerate(recommended_path):
                    node = skill_graph.nodes[skill_id]
                    status = "âœ… (ë³´ìœ )" if skill_id in my_skills else "ğŸ¯ (í•™ìŠµ í•„ìš”)"
                    print(f" Â Step {i+1}. {node['name']} (Level {node['level']}) {status}")

                print("="*70)

                print("\nğŸ’¡í˜„ì¬ ëŠ¥ë ¥ ìˆ˜ì¤€ì— ë§ëŠ” ê°•ì˜")
                print("="*70)

                courses = get_course_from_node(comp_unit_id, engine)

                for course in courses:
                    print(f"\t- {course}")

                print("="*70)

                print("\nğŸ’¡ìŠ¤í‚¬ ê°­(Skill Gap)")
                print("="*70)

                for skill_id in skill_gap:
                    node = skill_graph.nodes[skill_id]
                    print(f"{node['name']} (Level {node['level']})")
                    courses = get_course_from_node(skill_id, engine)
                    
                    for course in courses:
                        print(f"\t- {course}")

                    print("\n")
                print("="*70)

            else:
                print("ë¶„ì„ ì‹¤íŒ¨: ëª©í‘œ ì—­ëŸ‰ê¹Œì§€ ë„ë‹¬ ê°€ëŠ¥í•œ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


class EmbeddingManager:
    def __init__(self, model_name: str = "nlpai-lab/KURE-v1"):
        self.model_name = model_name
        self.model = SentenceTransformer(self.model_name, device="cpu")

    async def get_avg_query_embedding(self, queries: list[str]) -> torch.Tensor:
        if not queries:
            raise ValueError("ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        all_query_embeddings: torch.Tensor = await self.embed_async(queries)

        if all_query_embeddings.nelement() == 0:
            raise ValueError("ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¡œë¶€í„° ìœ íš¨í•œ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if len(queries) > 1:
            final_query_embedding_tensor = torch.mean(all_query_embeddings, dim=0, keepdim=True)
        else:
            final_query_embedding_tensor = all_query_embeddings

        return final_query_embedding_tensor

    async def embed_async(self, texts: list[str]) -> torch.Tensor:
        if not texts:
            raise ValueError("TEST")

        try:
            # ì´ë²¤íŠ¸ ë£¨í”„ ë¸”ë¡œí‚¹ ë°©ì§€ë¥¼ ìœ„í•´ ë¹„ë™ê¸° ì‹¤í–‰
            embeddings = await asyncio.to_thread(
                self.model.encode,
                texts,
                show_progress_bar=False,
                convert_to_tensor=True
            )

            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

            return embeddings

        except Exception as e:
            raise ValueError("zzzz")


def load_data_from_db_with_embeddings(engine):
    try:
        query = """
            SELECT
            unit.comp_unit_id,
            unit.comp_unit_name,
            unit.comp_unit_def,
            unit.comp_unit_level,
            emb.embedding_unit_def
            FROM tb_ncs_comp_unit_emb emb
            JOIN tb_ncs_comp_unit unit
            ON emb.comp_unit_id = unit.comp_unit_id
            WHERE unit.useflag = 'Y' AND unit.use_ncs = 'Y'
        """

        df = pd.read_sql(query, engine)

        print(f"ì„±ê³µì ìœ¼ë¡œ {len(df)}ê±´ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

        # í…ìŠ¤íŠ¸ í˜•íƒœì˜ ì„ë² ë”©ì„ ì‹¤ì œ Numpy ë°°ì—´ë¡œ ë³€í™˜
        # ì˜ˆ: "[0.1,0.2,...]" -> np.array([0.1, 0.2, ...])
        # DB ì €ì¥ í˜•ì‹ì— ë”°ë¼ í›„ì²˜ë¦¬ ë°©ì‹(sep ë“±)ì„ ì¡°ì ˆí•´ì•¼ í•©ë‹ˆë‹¤.
        df['embedding_unit_def'] = df['embedding_unit_def'].apply(
            lambda x: np.fromstring(x.strip('[]'), sep=',')
        )

        return df

    except Exception as e:
        print(f"ë°ì´í„°ë² ì´ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


# ì—˜ë³´ìš° ë°©ë²•ìœ¼ë¡œ ìµœì ì˜ K ì°¾ê¸° (ëª‡ê°œì˜ êµ°ì§‘ìœ¼ë¡œ ë¬¶ì„ ê²ƒì¸ì§€)

def draw_graph_for_find_better_k(embedding_vectors):
    inertia_values = []
    k_range = range(10, 101, 10)

    print("ì—˜ë³´ìš° ë°©ë²•ì„ ìœ„í•œ K-Meansë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")

    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
        kmeans.fit(embedding_vectors)

        inertia_values.append(kmeans.inertia_)
        print(f"K={k} ì™„ë£Œ, Inertia(ì‘ì§‘ë„)={kmeans.inertia_:.2f}")

    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10, 6))
    plt.plot(k_range, inertia_values, marker='o')
    plt.title('Elbow Method for Optimal K')
    plt.xlabel('Number of clusters (K)')
    plt.ylabel('Inertia')
    plt.grid(True)
    plt.show()


if __name__ == "__main__":
    db_url = "postgresql://u_cufit:cufit23$@223.130.152.134:5432/curationdb"
    engine = create_engine(db_url)
    ncs_df = load_data_from_db_with_embeddings(engine)

    # Embedding ëª¨ë¸ ì´ˆê¸°í™”
    embedding_manager = EmbeddingManager()

    # FAISS ì´ˆê¸°í™”
    # 1. DBì—ì„œ ë¶ˆëŸ¬ì˜¨ ì„ë² ë”© ë²¡í„°ë¥¼ Faissê°€ ì‚¬ìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    #   - (ë°ì´í„° ê°œìˆ˜, ì„ë² ë”© ì°¨ì›) í˜•íƒœì˜ 2D Numpy ë°°ì—´
    #   - float32 ìë£Œí˜•
    embedding_unit_def = np.array(list(ncs_df['embedding_unit_def'])).astype("float32")
    embedding_dimension = embedding_unit_def.shape[1]

    # 2. Faiss ì¸ë±ìŠ¤ ìƒì„± ë° ë²¡í„° ì¶”ê°€
    # IndexFlatL2ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ L2 ê±°ë¦¬(ìœ í´ë¦¬ë“œ ê±°ë¦¬) ê¸°ë°˜ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ L2 ê±°ë¦¬ëŠ” ì •ê·œí™”ëœ(normalized) ë²¡í„°ì—ì„œëŠ” ë™ì¼í•œ ìˆœì„œë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    quantizer = faiss.IndexFlatIP(embedding_dimension)
    nlist = int(np.sqrt(len(embedding_unit_def)))
    index = faiss.IndexIVFFlat(quantizer, embedding_dimension, nlist, faiss.METRIC_INNER_PRODUCT)

    print("Faiss ì¸ë±ìŠ¤ë¥¼ í•™ìŠµ(training)í•©ë‹ˆë‹¤...")
    index.train(embedding_unit_def)

    print("Faiss ì¸ë±ìŠ¤ì— ë²¡í„°ë¥¼ ì¶”ê°€(add)í•©ë‹ˆë‹¤...")
    index.add(embedding_unit_def)

    # nprobe íŒŒë¼ë¯¸í„° ì„¤ì •
    index.nprobe = n_probes

    print(f"Faiss ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ. ì´ {index.ntotal}ê°œì˜ ë²¡í„°ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")



    # https://bommbom.tistory.com/entry/%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0%EB%A7%81-%EC%B5%9C%EC%A0%81-%EA%B5%B0%EC%A7%91-%EC%88%98-%EC%97%98%EB%B3%B4%EC%9A%B0-vs-%EC%8B%A4%EB%A3%A8%EC%97%A3-%EA%B8%B0%EB%B2%95
    # draw_graph_for_find_better_k(ncs_emb_vectors)
    OPTIMAL_K = 20
    kmeans = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init="auto")
    kmeans.fit(embedding_unit_def)

    # 3. ê° ë°ì´í„° í¬ì¸íŠ¸ì— í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”(ê¼¬ë¦¬í‘œ) ê°€ì ¸ì˜¤ê¸°
    cluster_labels = kmeans.labels_ # [0, 15, 4, 15, 22, ...] ì™€ ê°™ì€ ë°°ì—´

    # 4. ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— 'cluster_id' ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
    ncs_df['cluster_id'] = cluster_labels
    print("ë°ì´í„°í”„ë ˆì„ì— í´ëŸ¬ìŠ¤í„° IDê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(ncs_df[['comp_unit_id', 'cluster_id']].head())

    # ----- TEST -----

    # 1. ì‚¬ìš©ì í‚¤ì›Œë“œ ì…ë ¥
    target_goal = "íŒŒì´ì¬ì„ í™œìš©í•œ ì¸ê³µì§€ëŠ¥ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ ê³¼ì •"
    top_k = 10

    # 2. í‚¤ì›Œë“œì™€ NCS ëŠ¥ë ¥ë‹¨ìœ„ ë¹„êµ
    """1ë‹¨ê³„: ëª©í‘œ í‚¤ì›Œë“œì™€ ìœ ì‚¬í•œ ëŠ¥ë ¥ë‹¨ìœ„ ì¶”ì¶œ"""
    print(f"'{target_goal}'ì™€ ìœ ì‚¬í•œ ëŠ¥ë ¥ë‹¨ìœ„ {top_k}ê°œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")

    # ì‚¬ìš©ì í‚¤ì›Œë“œë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜
    query_vector = embedding_manager.encode([target_goal]).astype('float32')

    # Faissë¥¼ ì‚¬ìš©í•´ ê°€ì¥ ìœ ì‚¬í•œ Kê°œì˜ ëŠ¥ë ¥ë‹¨ìœ„ ì¸ë±ìŠ¤ë¥¼ ê²€ìƒ‰
    _, indices = index.search(query_vector, top_k)

    # ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œ
    relevant_indices = indices[0]
    relevant_units_df = ncs_df.iloc[relevant_indices].copy().sort_values(by='comp_unit_level')

    print("ì •ë ¬ëœ ëŠ¥ë ¥ë‹¨ìœ„ë¥¼ ì£¼ì œ(í´ëŸ¬ìŠ¤í„°)ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ëª¨ë“ˆì„ êµ¬ì„±í•©ë‹ˆë‹¤...")

    # defaultdictë¥¼ ì‚¬ìš©í•˜ë©´ í‚¤ê°€ ì—†ì„ ë•Œ ìë™ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•´ í¸ë¦¬í•¨
    modules = defaultdict(list)

    # DataFrameì„ ìˆœíšŒí•˜ë©° cluster_nameì„ í‚¤ë¡œ í•˜ì—¬ ëŠ¥ë ¥ë‹¨ìœ„ ì •ë³´ ì¶”ê°€
    for _, row in relevant_units_df.iterrows():
        module_name = row['cluster_name']
        unit_info = f"{row['comp_unit_name']} (Level {row['comp_unit_level']})"
        modules[module_name].append(unit_info)



    # 4. ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print(f"ğŸ¯ ëª©í‘œ: {target_goal}")
    print("ğŸ’¡ AIê°€ ìƒì„±í•œ êµìœ¡ê³¼ì • ì´ˆì•ˆì…ë‹ˆë‹¤.")
    print("="*60)

    # ëª¨ë“ˆ ì´ë¦„(í´ëŸ¬ìŠ¤í„° ì´ë¦„)ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
    for module_name in sorted(generated_curriculum.keys()):
        print(f"\nğŸ“˜ Module: {module_name}")
        for unit in generated_curriculum[module_name]:
            print(f" Â  - {unit}")

    print("="*60)