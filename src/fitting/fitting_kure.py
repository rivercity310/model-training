import os
import json
import torch
import random
from dataclasses import dataclass
from dotenv import load_dotenv
from torch.utils.data import DataLoader, IterableDataset
from sentence_transformers import SentenceTransformer, losses, util
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.readers import InputExample
from datetime import datetime
from peft import LoraConfig, get_peft_model, PeftModel
from pathlib import Path

# GPU ì§€ì› Pytorch ì„¤ì¹˜
# https://pytorch.org/get-started/locally/#slide-out-widget-area
# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìƒìˆ˜
EMB_TB_NM = "tb_ncs_comp_unit_emb_test"
KURE_DATASET_GLOB = "kure_train_dataset_*.json"


@dataclass(frozen=True)
class Paths:
    # í´ë” ê²½ë¡œ
    ROOT = Path(os.getenv("PROJECT_ROOT_DIR")).resolve()
    CSV = ROOT / "data" / "csv"
    ANALYSIS = ROOT / "data" / "analysis"
    JSON = ROOT / "data" / "json"
    MODEL_OUTPUT = ROOT / "output"
    KURE_DATASET = JSON / "ncs"

    # íŒŒì¼ ê²½ë¡œ
    F_EMB_CSV = CSV / f"{EMB_TB_NM}.csv"

    @classmethod
    def get_kure_dataset_json(cls, batch_num: int) -> Path:
        return cls.JSON / "ncs" / f"kure_train_dataset_{batch_num}.json"


# ê²½ë¡œ ì •ì˜
DATE = datetime.now().strftime("%y_%m_%d_%H_%M_%S")
MODEL_OUTPUT_PATH = Paths.MODEL_OUTPUT / f"kure_finetuned_{DATE}"


if not MODEL_OUTPUT_PATH.exists():
    MODEL_OUTPUT_PATH.mkdir(parents=True)

# GPU ì •ë³´
is_available = torch.cuda.is_available()
print(f"GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {is_available}")

# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ë©´, ì¥ì¹˜ ì´ë¦„ ì¶œë ¥
if is_available:
    print(f"GPU ì¥ì¹˜ ìˆ˜: {torch.cuda.device_count()}")
    print(f"í˜„ì¬ GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")


def load_ncs_dataset(path):
    """JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ëª¨ë¸ í•™ìŠµì— ë§ëŠ” InputExample ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘í™˜"""
    train_examples = []
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        for item in data:
            query = item["query"]
            positive_doc = item["positive_document"]
            similarity = float(item["similarity"])
            target = f"{positive_doc['ncs_title']}: {positive_doc['ncs_description']}"
            train_examples.append(InputExample(texts=[query, target], label=similarity))

    return train_examples


class NcsStreamDataset(IterableDataset):
    def __init__(self, filepaths):
        super().__init__()
        self.filepaths = filepaths

    def __iter__(self):
        # ë§¤ ì—í­ë§ˆë‹¤ íŒŒì¼ ìˆœì„œë¥¼ ì„ì–´ì£¼ë©´ í•™ìŠµ íš¨ê³¼ í–¥ìƒ
        random.shuffle(self.filepaths)

        # ê° íŒŒì¼ ê²½ë¡œì— ëŒ€í•´ ë°˜ë³µ
        for filepath in self.filepaths:
            # íŒŒì¼ì—ì„œ ìƒ˜í”Œë“¤ì„ ë¡œë“œ (ì´ë•ŒëŠ” íŒŒì¼ í•˜ë‚˜ ë¶„ëŸ‰ë§Œ ë©”ëª¨ë¦¬ì— ì˜¬ë¼ê°)
            samples = load_ncs_dataset(filepath)

            # í•´ë‹¹ íŒŒì¼ì˜ ê° ìƒ˜í”Œì„ í•˜ë‚˜ì”© ë°˜í™˜
            for sample in samples:
                print(sample)
                yield sample


def train() -> bool:
    if not is_available:
        print("WARN: GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")

    # ëª¨ë“  í›ˆë ¨ ë°ì´í„°ì…‹ì„ ë©”ëª¨ë¦¬ì— ë¡œë”©
    all_filepaths = sorted(list(Paths.KURE_DATASET.glob(KURE_DATASET_GLOB)))

    if not all_filepaths:
        print("í•™ìŠµ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False

    # ë°ì´í„°ë¥¼ í›ˆë ¨ìš© 90%, ê²€ì¦ìš© 10%ë¡œ ë¶„í• 
    train_size = int(len(all_filepaths) * 0.9)
    train_filepaths = all_filepaths[:train_size]
    eval_filepaths = all_filepaths[train_size:]
    print(
        f"[ë°ì´í„°] ì „ì²´ íŒŒì¼ ìˆ˜: {len(all_filepaths)}, í›ˆë ¨ íŒŒì¼ ìˆ˜: {len(train_filepaths)}, ê²€ì¦ íŒŒì¼ ìˆ˜: {len(eval_filepaths)}"
    )

    # í›ˆë ¨ ë°ì´í„°ìš© IterableDataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    train_dataset = NcsStreamDataset(train_filepaths)

    # ê²€ì¦ ë°ì´í„°ëŠ” Evaluatorë¥¼ ìœ„í•´ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    # - ê²€ì¦ ë°ì´í„°ëŠ” ë³´í†µ í¬ê¸°ê°€ ì‘ìœ¼ë¯€ë¡œ ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì ìŒ
    eval_samples = []
    for filepath in eval_filepaths:
        eval_samples.extend(load_ncs_dataset(filepath))

    if not eval_samples:
        print(
            "ê²€ì¦ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•˜ì§€ë§Œ, ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ ë° ìë™ ì €ì¥ì€ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        )
        evaluator = None
    else:
        print(f"[ë°ì´í„°] ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(eval_samples)}")
        evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
            eval_samples, name="ncs-eval"
        )

    # ëª¨ë¸
    model_name = os.getenv("EMBEDDING_MODEL")
    model = SentenceTransformer(model_name)
    print(f"[ëª¨ë¸] {model_name} ëª¨ë¸ì˜ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # LoRa ì ìš©
    # - SentenceTransformer ë‚´ë¶€ Transformer ëª¨ë¸ì— LoRa ì ìš©
    # - ì¼ë°˜ì ìœ¼ë¡œ ì–´í…ì…˜ ë ˆì´ì–´ì˜ query, key, value í”„ë¡œì ì…˜ì— ì ìš©
    lora_config = LoraConfig(
        r=16,                        # Rank: 8, 16, 32.. -> ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥ê³¼ íŒŒë¼ë¯¸í„° ìˆ˜ ì¦ê°€
        lora_alpha=32,               # LoRa Scailing Factor: ì¼ë°˜ì ìœ¼ë¡œ Rank * 2
        lora_dropout=0.05,
        bias="none",
        task_type="FEATURE_EXTRACTION"
    )

    # ëª¨ë¸ì— LoRa ì–´ëŒ‘í„° ì¶”ê°€
    # - SentenceTransformer ëª¨ë¸ì˜ Transformer ë¶€ë¶„ì— PEFT ëª¨ë¸ ì ìš©
    model[0].auto_model = get_peft_model(model[0].auto_model, lora_config)
    print("[LoRa] LoRa ì–´ëŒ‘í„°ë¥¼ ëª¨ë¸ì— ì ìš©í–ˆìŠµë‹ˆë‹¤.")
    model[0].auto_model.print_trainable_parameters()     # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥

    # í›ˆë ¨ íŒŒë¼ë¯¸í„°
    # - Streaming ë°ì´í„°ì…‹ì€ ì „ì²´ ê¸¸ì´ë¥¼ ë¯¸ë¦¬ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ warmupê³¼ evaluation stepì„ ê³ ì •ëœ ê°’ìœ¼ë¡œ ì„¤ì •
    epochs = 4
    learning_rate = 2e-5
    train_batch_size = 16
    warmup_steps = 500
    evaluation_steps = 1000

    # v2.x ì—ì„œëŠ” DataLoaderë¥¼ ì§ì ‘ ìƒì„±
    # DataLoaderëŠ” ì´ì œ IterableDatasetìœ¼ë¡œë¶€í„° ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•¨
    # IterableDatasetì€ shuffle=True ì˜µì…˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ (í•„ìš” ì‹œ Dataset ë‚´ë¶€ì—ì„œ êµ¬í˜„)
    train_dataloader = DataLoader(train_dataset, batch_size=train_batch_size)

    # ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
    # 1. MultipleNegativesRankingLoss
    # - ê¸ì •ìŒì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ (ì§ˆë¬¸, ë‹¤ë¥¸NCS) ìŒë“¤ì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶”ë„ë¡ í•™ìŠµ -> ê²€ìƒ‰/ì¶”ì²œì— ë§¤ìš° íš¨ê³¼ì 
    # 2. CosineSimilarityLoss
    # - 0.95ì ì§œë¦¬ ìŒì€ ë²¡í„° ê³µê°„ì—ì„œ ë§¤ìš° ê°€ê¹ê²Œ, 0.7ì ì§œë¦¬ ìŒì€ ì ë‹¹íˆ ê°€ê¹ê²Œ ë°°ì¹˜í•˜ë„ë¡ í•™ìŠµí•˜ì—¬ ê´€ê³„ì˜ 'ì •ë„'ë¥¼ í•™ìŠµ
    train_loss = losses.CosineSimilarityLoss(model)

    # - í›ˆë ¨ ìŠ¤í…ì˜ 10%ë¥¼ ì›Œë°ì—…ìœ¼ë¡œ ì‚¬ìš©
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        evaluator=evaluator,
        epochs=epochs,
        evaluation_steps=evaluation_steps,
        warmup_steps=warmup_steps,
        output_path=str(MODEL_OUTPUT_PATH),
        optimizer_params={"lr": learning_rate},
        show_progress_bar=True,
    )

    # ëª¨ë¸ í•™ìŠµ ì‹¤í–‰
    print("ğŸ‰ ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    # - ì „ì²´ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ëŒ€ì‹  ì–´ëŒ‘í„°ë§Œ ì €ì¥
    MODEL_OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
    lora_adapter_path = MODEL_OUTPUT_PATH / "lora_adapter"
    model[0].auto_model.save_pretrained(str(lora_adapter_path))
    print(f"LoRa ì–´ëŒ‘í„°ê°€ {lora_adapter_path} ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return True


if __name__ == "__main__":
    success = train()

    if not success:
        print("í›ˆë ¨ ì‹¤íŒ¨")
        exit(1)

    # í›ˆë ¨ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    base_model_name = os.getenv("EMBEDDING_MODEL")
    finetuned_model = SentenceTransformer(base_model_name)
    lora_adapter_path = MODEL_OUTPUT_PATH / "lora_adapter"

    # ì›ë³¸ ëª¨ë¸ì— LoRa ì–´ëŒ‘í„° ê²°í•©
    finetuned_model[0].auto_model = PeftModel.from_pretrained(
        finetuned_model[0].auto_model, 
        str(lora_adapter_path)
    )
    
    print(f"\n[ì¶”ë¡ ] ì›ë³¸ ëª¨ë¸({base_model_name})ì— LoRa ì–´ëŒ‘í„°({lora_adapter_path})ë¥¼ ê²°í•©í–ˆìŠµë‹ˆë‹¤.")

    # ê²€ìƒ‰ ëŒ€ìƒì´ ë  NCS ì§ë¬´ ì •ë³´
    corpus_docs_data = []
    batch_num = 1

    while True:
        json_path = Paths.get_kure_dataset_json(batch_num)

        if not json_path.exists():
            break

        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            for item in data:
                corpus_docs_data.append(
                    {
                        "code": item["positive_document"]["ncs_code"],
                        "content": f"{item['positive_document']['ncs_title']}: {item['positive_document']['ncs_description']}]",
                    }
                )

        corpus_contents = [doc["content"] for doc in corpus_docs_data]

        # Corpusë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        corpus_embeddings = finetuned_model.encode(
            corpus_contents, convert_to_tensor=True, show_progress_bar=True
        )

        # í…ŒìŠ¤íŠ¸ ì§ˆì˜
        while True:
            test_query = input("Query: ")
            test_query_embedding = finetuned_model.encode(
                test_query, convert_to_tensor=True
            )

            # ì˜ë¯¸ë¡ ì  ê²€ìƒ‰
            hits = util.semantic_search(
                test_query_embedding, corpus_embeddings, top_k=3
            )[0]

            # ê²°ê³¼ ì¶œë ¥
            print(f'\n--- í…ŒìŠ¤íŠ¸ ì§ˆì˜: "{test_query}" ---')
            print("ê°€ì¥ ìœ ì‚¬í•œ NCS ì§ë¬´ TOP 3:")
            for hit in hits:
                doc_index = hit["corpus_id"]
                score = hit["score"]
                ncs_code = corpus_docs_data[doc_index]["code"]
                doc_content = corpus_docs_data[doc_index]["content"]

                print(f"  - NCS ì½”ë“œ: {ncs_code} (ìœ ì‚¬ë„: {score:.4f})")
                print(f"    ë‚´ìš©: {doc_content}\n")
