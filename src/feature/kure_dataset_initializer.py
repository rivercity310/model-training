import pandas as pd
import time
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from pydantic import BaseModel, Field
from tqdm import tqdm
from pathlib import Path
from langchain.prompts import PromptTemplate
from langchain.chat_models import init_chat_model
from langchain.output_parsers import PydanticOutputParser
from langchain.schema.output_parser import StrOutputParser
from src.util import (
    Paths,
    EMB_TB_NM,
    KURE_DATASET_GLOB,
    initialize_ncs,
    parse_json_from_text,
)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜
API_KEY = os.getenv("LLM_API_KEY")
MODEL_NAME = os.getenv("LLM_MODEL_NAME")
MODEL_PROVIDER = os.getenv("LLM_MODEL_PROVIDER")
IS_GEMINI = MODEL_PROVIDER == "google_genai"

# API í‚¤ ì„¸íŒ…
if "google" in MODEL_PROVIDER and not os.environ.get("GOOGLE_API_KEY"):
    os.environ["GOOGLE_API_KEY"] = API_KEY

elif "openai" in MODEL_PROVIDER and not os.environ.get("OPENAI_API_KEY"):
    os.environ["OPENAI_API_KEY"] = API_KEY


# --- Pydantic ëª¨ë¸ ì •ì˜ ---
class Question(BaseModel):
    input: str = Field(description="NCS ì§ë¬´ì™€ ê´€ë ¨ëœ ì˜ˆìƒ ì‚¬ìš©ì ì§ˆë¬¸")
    similarity: float = Field(
        description="ì˜ˆìƒ ì§ˆë¬¸ê³¼ NCS ì§ë¬´ ì •ë³´ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì ìˆ˜ (0.5ì—ì„œ 1.0 ì‚¬ì´)"
    )


class GeneratedQuestionList(BaseModel):
    questions: list[Question] = Field(description="ìƒì„±ëœ ì§ˆë¬¸ ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸")


class NCSDatasetGenerator:
    def __init__(self):
        _chain, _parser = self._create_chain()
        self.chain = _chain
        self.parser = _parser

    @staticmethod
    def _create_chain():
        """LangChain êµ¬ì„± ìš”ì†Œ ì„¸íŒ… ë° ì™„ì„±ëœ ì²´ì¸ ìƒì„±"""
        print("LangChain ì²´ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

        # ëª¨ë¸ ì´ˆê¸°í™”
        model = init_chat_model(model=MODEL_NAME, model_provider=MODEL_PROVIDER)

        # íŒŒì„œ ì´ˆê¸°í™”
        parser = PydanticOutputParser(pydantic_object=GeneratedQuestionList)

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (í¬ë§· ì§€ì¹¨ í¬í•¨)
        format_instructions = parser.get_format_instructions()

        template_string = """
        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì ì¬ì  ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì£¼ì–´ì§„ NCS ì§ë¬´ì •ë³´ë¥¼ ë³´ê³ , ì‚¬ìš©ìë“¤ì´ ì–´ë–¤ ì§ˆë¬¸ì„ í• ì§€ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.
        ì‚¬ìš©ìëŠ” ì£¼ë¡œ ìì‹ ì´ ê¶ê¸ˆí•œ ë¶„ì•¼ì— ëŒ€í•œ ê°•ì˜ë¥¼ ì°¾ê¸° ìœ„í•´ ì§ˆë¬¸í•˜ë¯€ë¡œ, í•™ìŠµ ë° ì§ë¬´ ì—­ëŸ‰ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
        ìµœì†Œ 7ê°œ ì´ìƒì˜ ë‹¤ì–‘í•œ ì§ˆë¬¸ì„ ìƒì„±í•˜ê³ , ê° ì§ˆë¬¸ê³¼ NCS ì§ë¬´ ê°„ì˜ ì˜ˆìƒ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”.
        ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0.5 ë¯¸ë§Œì¸ ì§ˆë¬¸ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.

        ## NCS ì§ë¬´ì •ë³´
        - ì§ë¬´ëª…: {ncs_name}
        - ì§ë¬´ ìˆ˜ì¤€: {ncs_level}
        - ì§ë¬´ ì„¤ëª…: {ncs_desc}

        ## ì¶œë ¥ í˜•ì‹ ì§€ì¹¨
        {format_instructions}
        """

        prompt_template = PromptTemplate(
            template=template_string,
            input_variables=["ncs_name", "ncs_level", "ncs_desc"],
            partial_variables={"format_instructions": format_instructions},
        )

        return prompt_template | model | StrOutputParser(), parser

    def generate_for_ncs_item(self, ncs_name: str, ncs_level: int, ncs_desc: str):
        """ë‹¨ì¼ NCS í•­ëª©ì— ëŒ€í•´ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        try:
            response_text = self.chain.invoke(
                {"ncs_name": ncs_name, "ncs_level": ncs_level, "ncs_desc": ncs_desc}
            )

            json_string = parse_json_from_text(response_text)

            if json_string:
                return self.parser.parse(json_string)
            else:
                print(f"WARN: ì‘ë‹µì—ì„œ JSONì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì§ë¬´: {ncs_name})")
                return None
        except Exception as e:
            print(f"ERROR: ì²´ì¸ ì‹¤í–‰ ë˜ëŠ” íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ. (ì§ë¬´: {ncs_name}) - {e}")
            return None


def save_dataset(filepath: Path, data_to_append: list):
    """
    JSON íŒŒì¼ì— ë°ì´í„°ë¥¼ ì´ì–´ì“°ê±°ë‚˜ ìƒˆë¡œ ìƒì„±

    Args:
        filepath (Path): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        data_to_append (list): ì¶”ê°€í•  ë°ì´í„° (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
    """
    if not data_to_append:
        return

    try:
        if not filepath.parent.exists():
            filepath.parent.mkdir()

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data_to_append, f, ensure_ascii=False, indent=2)

        print(
            f"\nâœ… {len(data_to_append)}ê°œ í•­ëª©ì„ '{filepath}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        print(f"\nâŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def get_processed_ids() -> set[str]:
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì´ë¯¸ ì²˜ë¦¬ëœ NCS ì½”ë“œ IDë“¤ì„ setìœ¼ë¡œ ë°˜í™˜"""
    output_dir = Paths.KURE_DATASET
    processed_ids = set()

    if not output_dir.exists():
        return processed_ids

    # kure_dataset_*.json íŒ¨í„´ì„ ê°€ì§„ ëª¨ë“  íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤.
    for filepath in output_dir.glob(KURE_DATASET_GLOB):
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
                for item in data:
                    if (
                        "positive_document" in item
                        and "ncs_code" in item["positive_document"]
                    ):
                        processed_ids.add(item["positive_document"]["ncs_code"])
        except (json.JSONDecodeError, KeyError) as e:
            print(f"WARN: '{filepath}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê±´ë„ˆëœë‹ˆë‹¤. ({e})")
            continue

    return processed_ids


def get_batch_num() -> int:
    return len(list(Paths.KURE_DATASET.glob(KURE_DATASET_GLOB))) + 1


def process_row(row):
    """DataFrameì˜ í•œ í–‰ì„ ë°›ì•„ LLMì„ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ íŒŒì‹±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    code = row["comp_unit_id"]
    name = row["comp_unit_name"]
    level = row["comp_unit_level"]
    desc = row["comp_unit_def"]

    results = ncs_dataset_generator.generate_for_ncs_item(name, level, desc)

    if results is None:
        print(f"WARN: {name}({code}) ì§ë¬´ì˜ ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    records = []
    for question in results.questions:
        record = {
            "query": question.input,
            "positive_document": {
                "ncs_code": str(code),
                "ncs_title": str(name),
                "ncs_description": str(desc),
                "level": int(level),
            },
            "similarity": question.similarity,
        }
        records.append(record)

    return records


if __name__ == "__main__":
    print(f"ëª¨ë¸ ê³µê¸‰ì: {MODEL_PROVIDER}")
    ncs_dataset_generator = NCSDatasetGenerator()

    if not Paths.F_EMB_CSV.exists():
        initialize_ncs(EMB_TB_NM).to_csv(Paths.F_EMB_CSV, index=False)

    df = pd.read_csv(Paths.F_EMB_CSV)

    # ì´ë¯¸ ì²˜ë¦¬ëœ í•­ëª©ë“¤ ID ë¡œë“œ
    processed_ids = get_processed_ids()
    if processed_ids:
        print(
            f"INFO: ê¸°ì¡´ì— ì²˜ë¦¬ëœ {len(processed_ids)}ê°œì˜ í•­ëª©ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤."
        )

    # ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë°ì´í„°ë§Œ í•„í„°ë§
    unprocessed_df = df[~df["comp_unit_id"].astype(str).isin(processed_ids)].copy()

    if unprocessed_df.empty:
        print("ğŸ‰ ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()

    print(f"ì´ {len(df)}ê°œ í•­ëª© ì¤‘, ìƒˆë¡œ ì²˜ë¦¬í•  í•­ëª©ì€ {len(unprocessed_df)}ê°œ ì…ë‹ˆë‹¤.")

    # LLM í˜¸ì¶œ ì œì–´ ë° ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
    CALLS_PER_MINUTE_LIMIT = 15
    SAVE_BATCH_SIZE = 50
    SLEEP_INTERVAL = 60 / CALLS_PER_MINUTE_LIMIT  # í˜¸ì¶œ ì‚¬ì´ì˜ ìµœì†Œ ëŒ€ê¸° ì‹œê°„
    MAX_WORKERS = 1 if IS_GEMINI else 10

    results_buffer = []
    call_counter = 0

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = []

        for index, row in unprocessed_df.iterrows():
            future = executor.submit(process_row, row)
            futures.append(future)

            if IS_GEMINI:
                time.sleep(SLEEP_INTERVAL)

        print(f"{len(futures)}ê°œì˜ ì‘ì—…ì„ ëª¨ë‘ ì œì¶œí–ˆìŠµë‹ˆë‹¤. ì´ì œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤...")

        for future in tqdm(
            as_completed(futures), total=len(futures), desc="NCS ë°ì´í„°ì…‹ ìƒì„± ì¤‘"
        ):
            records = future.result()

            if records is None:
                continue

            print(records)

            results_buffer.extend(records)
            call_counter += 1

            # Në²ˆ í˜¸ì¶œë§ˆë‹¤ íŒŒì¼ì— ì €ì¥
            if call_counter > 0 and call_counter % SAVE_BATCH_SIZE == 0:
                filepath = Paths.get_kure_dataset_json(get_batch_num())
                save_dataset(filepath, results_buffer)
                results_buffer.clear()

    # ë£¨í”„ê°€ ëë‚œ í›„ ë²„í¼ì— ë‚¨ì€ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ ì €ì¥
    if results_buffer:
        filepath = Paths.get_kure_dataset_json(get_batch_num())
        save_dataset(filepath, results_buffer)
        results_buffer.clear()

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
