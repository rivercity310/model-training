import pandas as pd
import time
import json
from tqdm import tqdm
from pathlib import Path
from src.util import Paths, EMB_TB_NM, initialize_ncs, parse_json_from_text
from src.infrastructure import model

# ë””ë²„ê·¸
DEBUG_MODE = True


def invoke(ncs_name, ncs_level, ncs_desc):
    json_format = """
    [
        {
            "input":        // ì‚¬ìš©ì ì§ˆë¬¸
            "similarity":   // ì‚¬ìš©ì ì§ˆë¬¸ê³¼ NCSì˜ ë§¤ì¹­ë¥ 
        },
        ...
    ]    
    """

    prompt = f"""
    ì•„ë˜ ì£¼ì–´ì§„ NCS ì§ë¬´ì •ë³´ë¥¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¤ì¹­ì‹œì¼œì•¼ í•´.
    ì‚¬ìš©ìëŠ” ì£¼ë¡œ ìì‹ ì´ ê¶ê¸ˆí•œ ë¶„ì•¼ì— ëŒ€í•œ ê°•ì˜ë¥¼ ì°¾ê¸° ìœ„í•´ ì§ˆë¬¸í•´.
    ì´ë¥¼ ê³ ë ¤í•´ì„œ ì˜ˆìƒë˜ëŠ” ì§ˆë¬¸ì„ ìµœì†Œ 7ê°œ ì´ìƒ ë½‘ì•„ì¤˜.
    ë§¤ì¹­ë¥ ì´ 0.5 ë¯¸ë§Œì¸ ì§ˆë¬¸ì€ ì‘ë‹µì—ì„œ ì œì™¸í•´ì¤˜.
    
    ## NCS ì§ë¬´ì •ë³´
    ì§ë¬´ëª…: {ncs_name}
    ì§ë¬´ ìˆ˜ì¤€: {ncs_level}
    ì§ë¬´ ì„¤ëª…: {ncs_desc}
    
    ## ì‘ë‹µ í˜•ì‹ (JSON)
    {json_format}
    """

    if DEBUG_MODE:
        print(prompt)

    return model.invoke(prompt).text()


# --- íŒŒì¼ ì €ì¥ í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
def save_dataset(filepath: Path, data_to_append: list):
    """
    JSON íŒŒì¼ì— ë°ì´í„°ë¥¼ ì´ì–´ì“°ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        filepath (Path): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        data_to_append (list): ì¶”ê°€í•  ë°ì´í„° (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
    """
    if not data_to_append:
        return

    try:
        if not filepath.parent.exists():
            filepath.parent.mkdir()

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_to_append, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… {len(data_to_append)}ê°œ í•­ëª©ì„ '{filepath}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    if not Paths.F_EMB_CSV.exists():
        initialize_ncs(EMB_TB_NM).to_csv(Paths.F_EMB_CSV, index=False)

    df = pd.read_csv(Paths.F_EMB_CSV)

    # LLM í˜¸ì¶œ ì œì–´ ë° ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
    CALLS_PER_MINUTE_LIMIT = 15
    SAVE_BATCH_SIZE = 50
    SLEEP_INTERVAL = 60 / CALLS_PER_MINUTE_LIMIT  # í˜¸ì¶œ ì‚¬ì´ì˜ ìµœì†Œ ëŒ€ê¸° ì‹œê°„

    results_buffer = []
    call_counter = 0

    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="NCS ë°ì´í„°ì…‹ ìƒì„± ì¤‘"):
        if call_counter > 0:
            time.sleep(SLEEP_INTERVAL)

        code = row['comp_unit_id']
        name = row['comp_unit_name']
        level = row['comp_unit_level']
        desc = row['comp_unit_def']

        res = invoke(name, level, desc)
        call_counter += 1

        json_res = parse_json_from_text(res)

        if json_res is None:
            print(f"WARN: {name}({code}) ì§ë¬´ì˜ ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        if DEBUG_MODE:
            print(json_res)

        # LLM ì‘ë‹µì„ ìµœì¢… ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë²„í¼ì— ì¶”ê°€
        for val in json_res:
            record = {
                "query": val['input'],
                "positive_document": {
                    "ncs_code": str(code),
                    "ncs_title": str(name),
                    "ncs_description": str(desc),
                    "level": int(level)
                },
                "similarity": float(val['similarity'])
            }

            results_buffer.append(record)

        # 3. 100ë²ˆ í˜¸ì¶œë§ˆë‹¤ íŒŒì¼ì— ì €ì¥ (Batch Saving)
        if call_counter % SAVE_BATCH_SIZE == 0:
            batch_num = call_counter // SAVE_BATCH_SIZE
            filepath = Paths.get_kure_dataset_json(batch_num)
            save_dataset(filepath, results_buffer)
            results_buffer.clear()

    if results_buffer:
        batch_num = (call_counter - 1) // SAVE_BATCH_SIZE + 1
        filepath = Paths.get_kure_dataset_json(batch_num)
        save_dataset(filepath, results_buffer)
        results_buffer.clear()

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")