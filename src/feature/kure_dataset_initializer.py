import pandas as pd
import time
import json
from tqdm import tqdm
from pathlib import Path
from src.util import Paths, EMB_TB_NM, initialize_ncs, parse_json_from_text
from src.infrastructure import invoke_ncs_datasest

# ë””ë²„ê·¸
DEBUG_MODE = True


# --- íŒŒì¼ ì €ì¥ í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€) ---
def save_dataset(filepath: Path, data_to_append: list):
    """
    JSON íŒŒì¼ì— ë°ì´í„°ë¥¼ ì´ì–´ì“°ê±°ë‚˜ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        filepath (Path): ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        data_to_append (list): ì¶”ê°€í•  ë°ì´í„° (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
    """
    if not data_to_append:
        return

    try:
        if not filepath.parent.exists():
            filepath.parent.mkdir()

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_to_append, f, ensure_ascii=False, indent=2)

        print(f"\nâœ… {len(data_to_append)}ê°œ í•­ëª©ì„ '{filepath}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    if not Paths.F_EMB_CSV.exists():
        initialize_ncs(EMB_TB_NM).to_csv(Paths.F_EMB_CSV, index=False)

    df = pd.read_csv(Paths.F_EMB_CSV)

    # LLM í˜¸ì¶œ ì œì–´ ë° ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
    CALLS_PER_MINUTE_LIMIT = 15
    SAVE_BATCH_SIZE = 50
    SLEEP_INTERVAL = 60 / CALLS_PER_MINUTE_LIMIT  # í˜¸ì¶œ ì‚¬ì´ì˜ ìµœì†Œ ëŒ€ê¸° ì‹œê°„

    results_buffer = []
    call_counter = 0

    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc="NCS ë°ì´í„°ì…‹ ìƒì„± ì¤‘"):
        if call_counter > 0:
            time.sleep(SLEEP_INTERVAL)

        code = row['comp_unit_id']
        name = row['comp_unit_name']
        level = row['comp_unit_level']
        desc = row['comp_unit_def']

        results = invoke_ncs_datasest(name, level, desc)
        call_counter += 1

        if results is None:
            print(f"WARN: {name}({code}) ì§ë¬´ì˜ ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        if DEBUG_MODE:
            print(results)

        # LLM ì‘ë‹µì„ ìµœì¢… ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë²„í¼ì— ì¶”ê°€
        for question in results.questions:
            record = {
                "query": question.input,
                "positive_document": {
                    "ncs_code": str(code),
                    "ncs_title": str(name),
                    "ncs_description": str(desc),
                    "level": int(level)
                },
                "similarity": question.similarity
            }

            results_buffer.append(record)

        # 3. 100ë²ˆ í˜¸ì¶œë§ˆë‹¤ íŒŒì¼ì— ì €ì¥ (Batch Saving)
        if call_counter % SAVE_BATCH_SIZE == 0:
            batch_num = call_counter // SAVE_BATCH_SIZE
            filepath = Paths.get_kure_dataset_json(batch_num)
            save_dataset(filepath, results_buffer)
            results_buffer.clear()

    if results_buffer:
        batch_num = (call_counter - 1) // SAVE_BATCH_SIZE + 1
        filepath = Paths.get_kure_dataset_json(batch_num)
        save_dataset(filepath, results_buffer)
        results_buffer.clear()

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")